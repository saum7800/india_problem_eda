{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wysa Data Extract.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Collection"
      ],
      "metadata": {
        "id": "i4HPJ0klH9ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task is to find out which Indian problems people are speaking about in social media. To this end, we will create a social media based dataset, perform an exploratory analysis, identify salient topics of discussion, and eventually create a model for predicting the topic given a social media post.\n",
        "\n",
        "In this notebook, our task is to collect data to form our dataset.\n",
        "\n",
        "Before we start collecting data, let us ask certain questions about how we wish to collect data.\n",
        "\n",
        "Q1) <strong>What platforms can we use?</strong>\n",
        "\n",
        "Popular social media platforms where opinions and problems have been discussed are Reddit and Twitter. While Reddit has longer written pieces and a more focused discussion on a post, Twitter is mostly short-form opinions without major discussion. Having tried to extract data from both, a preliminary look at the quality of data suggested that it is best to stick with a <strong>purely Reddit dataset</strong>.\n",
        "\n",
        "Q2) <strong>How do we ensure posts we collect are discussing an Indian Problem?</strong>\n",
        "\n",
        "First of all, to make sure that the broader topic of discussion is about India, we only look at India based subreddits like **r/India** and **r/IndiaSpeaks**. In order to further ensure that a problem is being discussed, we first try to find a flair (category on a subreddit) that is related to problems.\n",
        "\n",
        "For eg, r/IndiaSpeaks has the flair #Social-Issues on posts discussing problems.\n",
        "\n",
        "However, r/India does not have such a flair. In that case, we apply a filter on posts that only selects posts containing our keywords (\"Social\" and \"Problem\") or (\"Social\" and \"Issue\")\n",
        "\n",
        "Q3) **How do we collect reddit posts?**\n",
        "\n",
        "The official API offered by reddit for data scraping is praw. However, it offers little to no customizability and has terrible rate limits. A brilliant alternative is to use the Pushshift API, which allows for searching posts in a date range, according to particular search terms, etc.\n",
        "\n",
        "I am using a wrapper built around Pushshift called pmaw which allows for easier data collection.\n",
        "\n",
        "\n",
        "Other details like the date range we should enforce or the number of posts we should collect will be discussed ahead."
      ],
      "metadata": {
        "id": "Lv0IajvPIGAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6NlR8XIm8gV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import traceback"
      ],
      "metadata": {
        "id": "3IkyaVDUpZ4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmaw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnXYtw-ZDzCM",
        "outputId": "cbd853a2-a1e2-4ccf-82eb-1117ce7ef29b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmaw\n",
            "  Downloading pmaw-2.1.3-py3-none-any.whl (25 kB)\n",
            "Collecting praw\n",
            "  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pmaw) (2.23.0)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pmaw) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pmaw) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pmaw) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pmaw) (2021.10.8)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw, pmaw\n",
            "Successfully installed pmaw-2.1.3 praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_update_array(post):\n",
        "    title = post['title']\n",
        "    author = post['author']\n",
        "    created_utc = post['created_utc']\n",
        "    self_post = post['is_self']\n",
        "    score = post['score']\n",
        "    over_18 = post['over_18']\n",
        "    num_comments = post['num_comments']\n",
        "    \n",
        "    if 'is_original_content' not in post:\n",
        "        is_original_content = None\n",
        "    else:\n",
        "        is_original_content = post['is_original_content']\n",
        "    \n",
        "    if 'selftext' not in post:\n",
        "        self_text = \"\"\n",
        "    else:\n",
        "        self_text = post['selftext']\n",
        "    \n",
        "    if 'link_flair_text' not in post:\n",
        "        flair = None\n",
        "    else:\n",
        "        flair = post['link_flair_text']\n",
        "    \n",
        "    update_array = [title, flair, score, num_comments, author, is_original_content, created_utc, self_post, self_text, over_18]\n",
        "    return update_array"
      ],
      "metadata": {
        "id": "B0UHRhP_poYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_timestamp = int(datetime.utcnow().timestamp())\n",
        "url = \"https://api.pushshift.io/reddit/submission/search/?q={}&score=>0&before={}&after={}&sort_type=score&sort=desc&subreddit=India&limit=1000\"\n",
        "dataset = []\n",
        "epoch = start_timestamp\n",
        "year = 365*24*60*60\n",
        "epoch_prev = epoch - year\n",
        "search_terms = [\"Social Issues\", \"Social Issue\", \"Social Problem\", \"Social Problems\"]\n",
        "post_counts = 0\n",
        "total_years = 5\n",
        "for every_year in range(total_years):\n",
        "    for term in search_terms:\n",
        "        final_url = url.format(term, str(epoch),str(epoch_prev))\n",
        "        json_data = requests.get(final_url, headers={'User-Agent': \"test reddit app\"})\n",
        "        if json_data is None:\n",
        "            continue\n",
        "        print(json_data)\n",
        "        data = json_data.json()\n",
        "        posts = data['data']\n",
        "        for post in posts:\n",
        "            post_counts = post_counts + 1\n",
        "            update_array = get_update_array(post)\n",
        "            dataset.append(update_array)\n",
        "        epoch = epoch_prev\n",
        "        epoch_prev = epoch - year\n",
        "        print(post_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "3tCeOGtQpbVp",
        "outputId": "a1247f5e-70a7-42c5-ccf8-0528a7932948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n",
            "100\n",
            "<Response [200]>\n",
            "174\n",
            "<Response [200]>\n",
            "249\n",
            "<Response [200]>\n",
            "271\n",
            "<Response [200]>\n",
            "333\n",
            "<Response [200]>\n",
            "371\n",
            "<Response [200]>\n",
            "434\n",
            "<Response [200]>\n",
            "447\n",
            "<Response [200]>\n",
            "452\n",
            "<Response [429]>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3a93259de5b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##r/India Extraction\n",
        "\n",
        "My first attempt at directly using Pushshift failed due to exceeding rate limits almost immediately. This led me to look for [pmaw](https://github.com/mattpodolak/pmaw) which implements intelligent rate limiting.\n",
        "\n",
        "From the **r/India** subreddit, I determine the search terms social issue and social problem, and pull the top scoring 20,000 posts that contain either search term. I add the details received into the all_post_list"
      ],
      "metadata": {
        "id": "o_Nql8mzMzp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pmaw import PushshiftAPI\n",
        "import datetime as dt\n",
        "all_post_list = []\n",
        "api = PushshiftAPI()\n",
        "# before = dt.datetime(2022,5,1,0,0).timestamp()\n",
        "# month = 30*24*60*60\n",
        "# after = before - month*6\n",
        "search_terms = [\"Social Issue\", \"Social Problem\"]\n",
        "for term in search_terms:\n",
        "    posts = api.search_submissions(subreddit='India', q=term, sort_type=\"score\", sort=\"desc\", limit=20000)\n",
        "    all_post_list.extend([post for post in posts])\n",
        "    print(len(all_post_list))\n",
        "    # before = after\n",
        "    # after = after - month*6"
      ],
      "metadata": {
        "id": "W03tUYR5Gsmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d3765a-5012-41c2-be0b-79fb98ad535e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_india = pd.DataFrame(all_post_list)\n",
        "df_india.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LMpIYlgwj23",
        "outputId": "a6ed06da-c42f-4193-d7e3-7b5c7623e43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1015, 92)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find that Pushshift could only bring us 1000 posts that had those keywords. That's alright.\n",
        "\n",
        "Next, we check whether any duplicates seeped in that might have had both social issue and social problem in the same post."
      ],
      "metadata": {
        "id": "_rlt29v9N1dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_india['created_utc'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ici-sq7ywycP",
        "outputId": "f26491f9-9183-443f-c786-4762b59f9e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1579245405    6\n",
              "1636027326    6\n",
              "1579519599    5\n",
              "1636268366    5\n",
              "1533407177    5\n",
              "             ..\n",
              "1524470944    1\n",
              "1523774245    1\n",
              "1524320778    1\n",
              "1523746202    1\n",
              "1424591869    1\n",
              "Name: created_utc, Length: 857, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dup_rem_ind = df_india.drop_duplicates(subset=\"created_utc\")\n",
        "df_dup_rem_ind.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_J_eKJiw3c_",
        "outputId": "760492a0-1a0a-433e-f155-5888e9d4e4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(857, 92)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, they did. After removing duplicates, we have 857 posts from the r/India subreddit"
      ],
      "metadata": {
        "id": "bofkNreROGlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dup_rem_ind.to_pickle(\"df_india.pkl\")"
      ],
      "metadata": {
        "id": "iMUAeBdTbD-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##r/IndiaSpeaks extraction\n",
        "\n",
        "The next part of the code is going to extract data from the r/IndiaSpeaks subreddit.\n",
        "\n",
        "The way this is done is the following:\n",
        "\n",
        "1. We take periods of two months from now till the last 18 months.\n",
        "2. In each period, we take the top 10,000 scoring posts.\n",
        "3. From these posts, we add all those which have their flair as \"#Social-Issues\"\n",
        "\n",
        "The above method was chosen after quite a bit of trial and error. There are barely any posts with the social-issues flair beyond the 18months. Additionally, given the sparseness of social-issue posts, we have kept the limit for each 2 month period as top 10k posts."
      ],
      "metadata": {
        "id": "EMYRia8fOPRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flair_filter(item):\n",
        "    if 'link_flair_text' in item.keys():\n",
        "        if item[\"link_flair_text\"]==\"#Social-Issues 🗨️\":\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "5ZvKwyYppbWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pmaw import PushshiftAPI\n",
        "import datetime as dt\n",
        "all_post_list = []\n",
        "api = PushshiftAPI()\n",
        "before = int(dt.datetime(2022,5,1,0,0).timestamp())\n",
        "month = 30*24*60*60\n",
        "after = before - month*2\n",
        "for i in range(9):\n",
        "    print(before)\n",
        "    posts = api.search_submissions(subreddit='IndiaSpeaks', before=before, after=after, sort_type=\"score\", sort=\"desc\", limit=10000, filter_fn=flair_filter)\n",
        "    all_post_list.extend([post for post in posts])\n",
        "    print(len(all_post_list))\n",
        "    before = after\n",
        "    after = after - month*2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMnE9-2blqQc",
        "outputId": "fe507c32-0f67-41f8-befb-f9f34d867628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1651363200\n",
            "288\n",
            "1646179200\n",
            "677\n",
            "1640995200\n",
            "1122\n",
            "1635811200\n",
            "1536\n",
            "1630627200\n",
            "1791\n",
            "1625443200\n",
            "2043\n",
            "1620259200\n",
            "2105\n",
            "1615075200\n",
            "2105\n",
            "1609891200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Not all PushShift shards are active. Query results may be incomplete.\n",
            "Not all PushShift shards are active. Query results may be incomplete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_post = pd.DataFrame(all_post_list)"
      ],
      "metadata": {
        "id": "tNq3tZEno88l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_post.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHkz2EF3v41d",
        "outputId": "dffca8dd-8642-4c35-8b0d-64545e97a31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2105, 84)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_post['link_flair_text'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHY_by2yv8Dc",
        "outputId": "0e05b830-ecd7-444d-87fe-c1077b8d1a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "#Social-Issues 🗨️    2105\n",
              "Name: link_flair_text, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_post['created_utc'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4uVt2dPlREX",
        "outputId": "478c9872-39f5-4330-89da-fd9f0700c28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1644137529    9\n",
              "1644126217    9\n",
              "1644115796    8\n",
              "1644136176    8\n",
              "1619347455    7\n",
              "             ..\n",
              "1640589828    1\n",
              "1640617524    1\n",
              "1640623727    1\n",
              "1640777420    1\n",
              "1619234227    1\n",
              "Name: created_utc, Length: 1216, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dup_rem = df_all_post.drop_duplicates(subset=\"created_utc\")"
      ],
      "metadata": {
        "id": "_L_mcmormGVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dup_rem.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w2ZOZpZmyzE",
        "outputId": "6f53cc9a-d11d-4f66-b6a1-ee24876fee9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1216, 84)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dup_rem.to_pickle(\"df_ispeaks.pkl\")"
      ],
      "metadata": {
        "id": "j68NwkWJtH9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, after removing the duplicates from the IndiaSpeaks data as well, we are left with 1200 posts. Let us save this to pickle as well and do the data preprocessing for both datasets in the next part of our task!"
      ],
      "metadata": {
        "id": "_EKH2BQfPg45"
      }
    }
  ]
}